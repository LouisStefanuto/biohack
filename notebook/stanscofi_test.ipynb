{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from stanscofi.utils import load_dataset\n",
    "from stanscofi.datasets import Dataset\n",
    "from stanscofi.training_testing import cv_training\n",
    "from stanscofi.training_testing import weakly_correlated_split, random_simple_split, metrics_list\n",
    "from stanscofi.validation import compute_metrics, plot_metrics\n",
    "from stanscofi.validation import AUC, Rscore, MRR, RP, PrecisionK, RecallK, F1K, AP, MAP, DCGk, NDCGk, MeanRank, HRk, ERR\n",
    "\n",
    "from benchscofi import ALSWR, DRRS, LRSSL, PMF, SCPMF, Constant, LogisticMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1234\n",
    "decision_threshold = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset_di = load_dataset(\"TRANSCRIPT\", \"../data/\")\n",
    "dataset = Dataset(**dataset_di)\n",
    "\n",
    "# dataset_di = load_dataset(\"PREDICT\", \"../data/\")\n",
    "# dataset = Dataset(**dataset_di)\n",
    "\n",
    "dataset.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "metric = \"cosine\"\n",
    "\n",
    "# Random split\n",
    "(train_folds, test_folds), _ = random_simple_split(\n",
    "    dataset, test_size, metric=metric\n",
    ")\n",
    "\n",
    "# # Weakly correlated split\n",
    "# (train_folds, test_folds), _ = weakly_correlated_split(\n",
    "#     dataset,\n",
    "#     test_size,\n",
    "#     early_stop=1,\n",
    "#     metric=metric,\n",
    "#     verbose=True,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.subset(train_folds, subset_name=\"Train_\" + \"TRANSCRIPT\")\n",
    "test_dataset = dataset.subset(test_folds, subset_name=\"Test_\" + \"TRANSCRIPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train dataset\")\n",
    "train_dataset.summary()\n",
    "print(\"Test dataset\")\n",
    "test_dataset.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_params = {\n",
    "    \"PMF\": {\n",
    "        \"reg\": 0.01,\n",
    "        \"learning_rate\": 0.5,\n",
    "        \"n_iters\": 160,\n",
    "        \"n_factors\": 15,\n",
    "        \"batch_size\": 100,\n",
    "    },\n",
    "    \"LogisticMF\": {\n",
    "        \"counts\": np.zeros((63, 58)),\n",
    "        \"num_factors\": 2,\n",
    "    },\n",
    "    \"ALSWR\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm\n",
    "# model = PMF(algo_params[\"PMF\"])\n",
    "# model = LogisticMF(algo_params[\"LogisticMF\"])\n",
    "model = ALSWR(algo_params[\"ALSWR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dtype of the ratings matrix to float64\n",
    "dataset.ratings = dataset.ratings.astype(np.float64)\n",
    "# Train\n",
    "model.fit(dataset, random_state)\n",
    "\n",
    "# Predictions\n",
    "scores = model.predict_proba(test_dataset)\n",
    "predictions = model.predict(scores, threshold=decision_threshold)\n",
    "\n",
    "model.print_scores(scores)\n",
    "model.print_classification(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_di[\"ratings\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique_counts(dataset.ratings.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique_counts(predictions.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_pd = pd.DataFrame(predictions.todense())\n",
    "\n",
    "# Prediction index and columns should match the dataset_di[\"ratings\"]\n",
    "predictions_pd.index = dataset_di[\"ratings\"].index\n",
    "predictions_pd.columns = dataset_di[\"ratings\"].columns\n",
    "predictions_pd.to_csv(\"../results/predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model.fit(train_dataset, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "scores = model.predict_proba(test_dataset)\n",
    "predictions = model.predict(scores, threshold=decision_threshold)\n",
    "\n",
    "model.print_scores(scores)\n",
    "model.print_classification(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For validation\n",
    "k = 5\n",
    "beta = 1\n",
    "nsplits = 5\n",
    "njobs = nsplits - 1\n",
    "\n",
    "# Cross-validation\n",
    "results = cv_training(\n",
    "    ALSWR,\n",
    "    algo_params[\"ALSWR\"],\n",
    "    train_dataset,\n",
    "    threshold=decision_threshold,\n",
    "    metric=\"AUC\",\n",
    "    k=k,\n",
    "    beta=beta,\n",
    "    njobs=njobs,\n",
    "    nsplits=nsplits,\n",
    "    random_state=random_state,\n",
    "    show_plots=False,\n",
    "    verbose=False,\n",
    "    cv_type=\"random\",\n",
    "    # cv_type=\"weakly_correlated\"\n",
    ")\n",
    "model = results[\"models\"][np.argmax(results[\"test_metric\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "scores = model.predict_proba(test_dataset)\n",
    "predictions = model.predict(scores, threshold=decision_threshold)\n",
    "\n",
    "model.print_scores(scores)\n",
    "model.print_classification(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = test_dataset.ratings.toarray()\n",
    "np.unique_counts(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_array, csr_array\n",
    "x = predictions.toarray()\n",
    "np.unique_counts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "# disease-wise metrics\n",
    "metrics, plot_args = compute_metrics(\n",
    "    scores, predictions, test_dataset, metrics=metrics_list, k=k, beta=beta\n",
    ")\n",
    "# run all metrics\n",
    "plot_args.update({\"model_name\": \"LogisticMF\", \"figsize\": (8, 8)})\n",
    "plot_metrics(**plot_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset-wide metrics\n",
    "y_test = (test_dataset.folds.toarray() * test_dataset.ratings.toarray()).ravel()\n",
    "y_test[y_test < 1] = 0\n",
    "\n",
    "whole_metrics = [\n",
    "    AUC(y_test, scores.toarray().ravel(), k, beta),\n",
    "    Rscore(y_test, scores.toarray().ravel(), k, beta),\n",
    "    MRR(y_test, scores.toarray().ravel(), k, beta),\n",
    "    RP(y_test, scores.toarray().ravel(), k, beta),\n",
    "    PrecisionK(y_test, scores.toarray().ravel(), k, beta),\n",
    "    RecallK(y_test, scores.toarray().ravel(), k, beta),\n",
    "    F1K(y_test, scores.toarray().ravel(), k, beta),\n",
    "    AP(y_test, scores.toarray().ravel(), k, beta),\n",
    "    MAP(y_test, scores.toarray().ravel(), k, beta),\n",
    "    DCGk(y_test, scores.toarray().ravel(), k, beta),\n",
    "    NDCGk(y_test, scores.toarray().ravel(), k, beta),\n",
    "    MeanRank(y_test, scores.toarray().ravel(), k, beta),\n",
    "    HRk(y_test, scores.toarray().ravel(), k, beta),\n",
    "    ERR(y_test, scores.toarray().ravel(), k, beta),\n",
    "]\n",
    "\n",
    "results = pd.concat(\n",
    "    (\n",
    "        pd.DataFrame(\n",
    "            [whole_metrics],\n",
    "            index=[\"Value\"],\n",
    "            columns=[\n",
    "                \"AUC\",\n",
    "                \"Rscore\",\n",
    "                \"MRR\",\n",
    "                \"RP\",\n",
    "                \"PrecisionK\",\n",
    "                \"RecallK\",\n",
    "                \"F1K\",\n",
    "                \"AP\",\n",
    "                \"MAP\",\n",
    "                \"DCGk\",\n",
    "                \"NDCGk\",\n",
    "                \"MeanRank\",\n",
    "                \"HRk\",\n",
    "                \"ERR\",\n",
    "            ],\n",
    "        ).T,\n",
    "        metrics,\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
